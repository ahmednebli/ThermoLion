{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0HD8aOoIcwCO"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import GTSRB, SEMEION\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim.swa_utils import AveragedModel, SWALR\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import math\n",
        "import time\n",
        "import os\n",
        "\n",
        "# ====== Config ======\n",
        "BATCH_SIZE = 1024\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Running on: {DEVICE}\")\n",
        "print(f\"Batch Size: {BATCH_SIZE}\")\n",
        "\n",
        "# ============================================================\n",
        "# ====== OPTIMIZERS: CUSTOM IMPLEMENTATIONS ==================\n",
        "# ============================================================\n",
        "\n",
        "class Lion(optim.Optimizer):\n",
        "    \"\"\"\n",
        "    Lion Optimizer (Google Brain).\n",
        "    Sign-based, memory efficient.\n",
        "    \"\"\"\n",
        "    def __init__(self, params, lr=1e-4, betas=(0.9, 0.99), weight_decay=0.0):\n",
        "        defaults = dict(lr=lr, betas=betas, weight_decay=weight_decay)\n",
        "        super().__init__(params, defaults)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def step(self, closure=None):\n",
        "        for group in self.param_groups:\n",
        "            params = [p for p in group['params'] if p.grad is not None]\n",
        "            for p in params:\n",
        "                grad = p.grad\n",
        "                state = self.state[p]\n",
        "                if len(state) == 0:\n",
        "                    state['exp_avg'] = torch.zeros_like(p)\n",
        "\n",
        "                exp_avg = state['exp_avg']\n",
        "                beta1, beta2 = group['betas']\n",
        "                lr, wd = group['lr'], group['weight_decay']\n",
        "\n",
        "                # Update\n",
        "                update = exp_avg * beta1 + grad * (1 - beta1)\n",
        "                p.data.add_(torch.sign(update), alpha=-lr)\n",
        "\n",
        "                # Momentum Decay\n",
        "                exp_avg.mul_(beta2).add_(grad, alpha=1 - beta2)\n",
        "\n",
        "                # Weight Decay\n",
        "                if wd > 0:\n",
        "                    p.data.mul_(1 - lr * wd)\n",
        "\n",
        "class MuAdam(optim.Optimizer):\n",
        "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0):\n",
        "        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay)\n",
        "        super().__init__(params, defaults)\n",
        "    @torch.no_grad()\n",
        "    def step(self, closure=None):\n",
        "        for group in self.param_groups:\n",
        "            beta1, beta2 = group['betas']\n",
        "            for p in group['params']:\n",
        "                if p.grad is None: continue\n",
        "                grad = p.grad; state = self.state[p]\n",
        "                if len(state) == 0: state['step'] = 0; state['exp_avg'] = torch.zeros_like(p); state['exp_avg_sq'] = torch.zeros_like(p)\n",
        "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']; state['step'] += 1\n",
        "                if group['weight_decay'] != 0: p.mul_(1 - group['lr'] * group['weight_decay'])\n",
        "                exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)\n",
        "                exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)\n",
        "                denom = exp_avg_sq.sqrt().add_(group['eps'])\n",
        "                p.add_((exp_avg * beta1 + grad * (1-beta1)) / denom, alpha=-group['lr'])\n",
        "\n",
        "class ThermoLion(optim.Optimizer):\n",
        "    \"\"\"\n",
        "    Renamed from OmniThermoLion.\n",
        "    Thermodynamic-enhanced Lion optimization.\n",
        "    \"\"\"\n",
        "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.99), temp_decay=0.99, weight_decay=0.01):\n",
        "        defaults = dict(lr=lr, betas=betas, temp_decay=temp_decay, weight_decay=weight_decay)\n",
        "        super().__init__(params, defaults)\n",
        "    @torch.no_grad()\n",
        "    def step(self, closure=None):\n",
        "        for group in self.param_groups:\n",
        "            beta1, beta2 = group['betas']; lr = group['lr']; wd = group['weight_decay']; tdec = group['temp_decay']\n",
        "            for p in group['params']:\n",
        "                if p.grad is None: continue\n",
        "                g = p.grad.data; state = self.state[p]\n",
        "                if len(state) == 0: state['step'] = 0; state['m'] = torch.zeros_like(p); state['v'] = torch.zeros_like(p); state['temp'] = 1.0\n",
        "                m, v, temp = state['m'], state['v'], state['temp']; state['step'] += 1; temp *= tdec; state['temp'] = temp\n",
        "                m.mul_(beta1).add_(g, alpha=1-beta1)\n",
        "                v.mul_(beta2).addcmul_(g, g, value=1-beta2)\n",
        "                snr = torch.abs(m) / (torch.sqrt(v) + 1e-8)\n",
        "                gate = torch.tanh(snr)\n",
        "                step = (1 - gate) * torch.sign(m) * (1 + 0.5 * torch.clamp(torch.sign(m)*torch.sign(g), 0, 1)) * lr + gate * (m / (torch.sqrt(v) + 1e-8)) * lr * 2.0\n",
        "                if temp > 0.01: step += torch.randn_like(p) * math.sqrt(temp * v.mean().item() + 1e-10) * lr * (1 - gate)\n",
        "                if wd: p.mul_(1 - lr * wd)\n",
        "                p.add_(step, alpha=-1)\n",
        "\n",
        "# --- Lookahead Optimizer Wrapper (Fixed) ---\n",
        "class Lookahead(optim.Optimizer):\n",
        "    def __init__(self, optimizer, k=5, alpha=0.5):\n",
        "        self.optimizer = optimizer\n",
        "        self.k = k\n",
        "        self.alpha = alpha\n",
        "        self.param_groups = self.optimizer.param_groups\n",
        "        self.state = {}\n",
        "        # FIX: Initialize defaults from the inner optimizer\n",
        "        self.defaults = getattr(optimizer, 'defaults', {})\n",
        "        for group in self.param_groups:\n",
        "            group['counter'] = 0\n",
        "\n",
        "    def step(self, closure=None):\n",
        "        loss = self.optimizer.step(closure)\n",
        "        for group in self.param_groups:\n",
        "            if group['counter'] == 0:\n",
        "                for p in group['params']:\n",
        "                    self.state[p] = p.detach().clone()\n",
        "\n",
        "            group['counter'] += 1\n",
        "            if group['counter'] >= self.k:\n",
        "                group['counter'] = 0\n",
        "                for p in group['params']:\n",
        "                    slow = self.state[p]\n",
        "                    fast = p.data\n",
        "                    # Slow weights update: slow = slow + alpha * (fast - slow)\n",
        "                    fast.sub_(slow).mul_(self.alpha).add_(slow)\n",
        "                    slow.copy_(fast)\n",
        "        return loss\n",
        "\n",
        "    def zero_grad(self):\n",
        "        self.optimizer.zero_grad()\n",
        "\n",
        "# --- SWATS (Simplified) ---\n",
        "class SWATS(optim.Optimizer):\n",
        "    \"\"\"\n",
        "    Simplified SWATS: Starts as Adam, switches to SGD when condition met.\n",
        "    \"\"\"\n",
        "    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0):\n",
        "        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay, phase='ADAM')\n",
        "        super().__init__(params, defaults)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def step(self, closure=None):\n",
        "        for group in self.param_groups:\n",
        "            # Simple heuristic switch at step 300 for benchmark stability\n",
        "            for p in group['params']:\n",
        "                if p.grad is None: continue\n",
        "                grad = p.grad\n",
        "                state = self.state[p]\n",
        "                if len(state) == 0:\n",
        "                    state['step'] = 0\n",
        "                    state['exp_avg'] = torch.zeros_like(p)\n",
        "                    state['exp_avg_sq'] = torch.zeros_like(p)\n",
        "\n",
        "                state['step'] += 1\n",
        "\n",
        "                # Check switch condition\n",
        "                if state['step'] > 300:\n",
        "                    group['phase'] = 'SGD'\n",
        "\n",
        "                if group['phase'] == 'ADAM':\n",
        "                    # Adam Logic\n",
        "                    exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
        "                    beta1, beta2 = group['betas']\n",
        "                    exp_avg.mul_(beta1).add_(grad, alpha=1-beta1)\n",
        "                    exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1-beta2)\n",
        "                    denom = exp_avg_sq.sqrt().add_(group['eps'])\n",
        "                    step_size = group['lr'] * math.sqrt(1 - beta2 ** state['step']) / (1 - beta1 ** state['step'])\n",
        "                    p.addcdiv_(exp_avg, denom, value=-step_size)\n",
        "                else:\n",
        "                    # SGD Logic\n",
        "                    p.add_(grad, alpha=-group['lr'])\n",
        "\n",
        "                if group['weight_decay'] > 0:\n",
        "                    p.mul_(1 - group['lr'] * group['weight_decay'])\n",
        "\n",
        "# ==============================================\n",
        "# ====== DATASET & LOADING =====================\n",
        "# ==============================================\n",
        "\n",
        "def get_image_dataset(name):\n",
        "    \"\"\"\n",
        "    Returns (loader, model)\n",
        "    \"\"\"\n",
        "    root = './data'\n",
        "\n",
        "    # --- Transforms ---\n",
        "    transform_std = transforms.Compose([\n",
        "        transforms.Resize((32, 32)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "    ])\n",
        "\n",
        "    transform_gray = transforms.Compose([\n",
        "        transforms.Resize((32, 32)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5,), (0.5,))\n",
        "    ])\n",
        "\n",
        "    # Default Config\n",
        "    channels = 3\n",
        "    classes = 10\n",
        "\n",
        "    # --- NEW DATASETS (Replacements) ---\n",
        "    if name == \"GTSRB\":\n",
        "        ds = GTSRB(root=root, split='train', download=True, transform=transform_std)\n",
        "        channels = 3; classes = 43\n",
        "    elif name == \"SEMEION\":\n",
        "        ds = SEMEION(root=root, download=True, transform=transform_gray)\n",
        "        channels = 1; classes = 10\n",
        "\n",
        "    # --- Standard Datasets ---\n",
        "    elif name == \"MNIST\":\n",
        "        ds = torchvision.datasets.MNIST(root=root, train=True, download=True, transform=transform_gray); channels = 1\n",
        "    elif name == \"FashionMNIST\":\n",
        "        ds = torchvision.datasets.FashionMNIST(root=root, train=True, download=True, transform=transform_gray); channels = 1\n",
        "    elif name == \"KMNIST\":\n",
        "        ds = torchvision.datasets.KMNIST(root=root, train=True, download=True, transform=transform_gray); channels = 1\n",
        "    elif name == \"USPS\":\n",
        "        ds = torchvision.datasets.USPS(root=root, train=True, download=True, transform=transform_gray); channels = 1\n",
        "    elif name == \"QMNIST\":\n",
        "        ds = torchvision.datasets.QMNIST(root=root, what='train', download=True, transform=transform_gray); channels = 1\n",
        "    elif name == \"EMNIST\":\n",
        "        ds = torchvision.datasets.EMNIST(root=root, split='mnist', train=True, download=True, transform=transform_gray); channels = 1\n",
        "    elif name == \"CIFAR10\":\n",
        "        ds = torchvision.datasets.CIFAR10(root=root, train=True, download=True, transform=transform_std); channels = 3\n",
        "    elif name == \"CIFAR100\":\n",
        "        ds = torchvision.datasets.CIFAR100(root=root, train=True, download=True, transform=transform_std); channels = 3; classes = 100\n",
        "    elif name == \"SVHN\":\n",
        "        ds = torchvision.datasets.SVHN(root=root, split='train', download=True, transform=transform_std); channels = 3\n",
        "    elif name == \"STL10\":\n",
        "        ds = torchvision.datasets.STL10(root=root, split='train', download=True, transform=transform_std); channels = 3\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown dataset: {name}\")\n",
        "\n",
        "    # Limit samples for speed\n",
        "    limit = 5000\n",
        "    if len(ds) > limit:\n",
        "        indices = torch.arange(limit)\n",
        "        subset = torch.utils.data.Subset(ds, indices)\n",
        "    else:\n",
        "        subset = ds\n",
        "\n",
        "    loader = DataLoader(subset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "    # --- Models ---\n",
        "    model = nn.Sequential(\n",
        "        nn.Conv2d(channels, 32, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
        "        nn.Conv2d(32, 64, 3, padding=1), nn.ReLU(), nn.MaxPool2d(2),\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(64*8*8, 256), nn.ReLU(),\n",
        "        nn.Linear(256, classes)\n",
        "    ).to(DEVICE)\n",
        "\n",
        "    return loader, model\n",
        "\n",
        "def train_and_evaluate(dataset_name, opt_name):\n",
        "    try:\n",
        "        loader, model = get_image_dataset(dataset_name)\n",
        "    except Exception as e:\n",
        "        print(f\"Skipping {dataset_name}: {e}\")\n",
        "        return [], 0, 0.0, 999.9\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    lr = 1e-3\n",
        "\n",
        "    # SWA Handling\n",
        "    swa_model = None\n",
        "    swa_scheduler = None\n",
        "\n",
        "    # Select Optimizer\n",
        "    if opt_name == \"Adam\": opt = optim.Adam(model.parameters(), lr=lr)\n",
        "    elif opt_name == \"AdamW\": opt = optim.AdamW(model.parameters(), lr=lr)\n",
        "    elif opt_name == \"RMSprop\": opt = optim.RMSprop(model.parameters(), lr=lr)\n",
        "    elif opt_name == \"Lion\": opt = Lion(model.parameters(), lr=lr/3)\n",
        "    elif opt_name == \"MuAdam\": opt = MuAdam(model.parameters(), lr=lr)\n",
        "    elif opt_name == \"ThermoLion\": opt = ThermoLion(model.parameters(), lr=lr) # RENAMED\n",
        "    # New Optimizers\n",
        "    elif opt_name == \"Lookahead\":\n",
        "        base_opt = optim.Adam(model.parameters(), lr=lr)\n",
        "        opt = Lookahead(base_opt)\n",
        "    elif opt_name == \"SWATS\": opt = SWATS(model.parameters(), lr=lr)\n",
        "    elif opt_name == \"SWA\":\n",
        "        # SWA requires SGD usually + SWA Utils\n",
        "        opt = optim.SGD(model.parameters(), lr=lr * 10) # Higher LR for SGD/SWA\n",
        "        swa_model = AveragedModel(model)\n",
        "        swa_scheduler = SWALR(opt, swa_lr=0.01)\n",
        "    else: opt = optim.SGD(model.parameters(), lr=lr)\n",
        "\n",
        "    epochs = 12\n",
        "    losses = []\n",
        "\n",
        "    model.train()\n",
        "    start_time = time.time()\n",
        "    for ep in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        for x, y in loader:\n",
        "            x, y = x.to(DEVICE), y.to(DEVICE)\n",
        "            opt.zero_grad()\n",
        "            pred = model(x)\n",
        "            loss = criterion(pred, y)\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        # SWA Update Step at end of epoch\n",
        "        if opt_name == \"SWA\" and ep > 5:\n",
        "            swa_model.update_parameters(model)\n",
        "            swa_scheduler.step()\n",
        "\n",
        "        avg_loss = running_loss / len(loader)\n",
        "        losses.append(avg_loss)\n",
        "\n",
        "    # If SWA, update BN and use swa_model for eval\n",
        "    if opt_name == \"SWA\":\n",
        "        torch.optim.swa_utils.update_bn(loader, swa_model, device=DEVICE)\n",
        "        eval_model = swa_model\n",
        "    else:\n",
        "        eval_model = model\n",
        "\n",
        "    end_time = time.time()\n",
        "    duration = end_time - start_time\n",
        "    final_loss = losses[-1] if len(losses) > 0 else 0.0\n",
        "\n",
        "    # Eval\n",
        "    eval_model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            x, y = x.to(DEVICE), y.to(DEVICE)\n",
        "            pred = eval_model(x)\n",
        "            _, predicted = torch.max(pred.data, 1)\n",
        "            total += y.size(0)\n",
        "            correct += (predicted == y).sum().item()\n",
        "\n",
        "    acc = 100 * correct / total\n",
        "    return losses, duration, acc, final_loss\n",
        "\n",
        "# ==============================================\n",
        "# ====== BENCHMARK EXECUTION ===================\n",
        "# ==============================================\n",
        "\n",
        "# New datasets first\n",
        "datasets = [\n",
        "    \"GTSRB\", \"SEMEION\",\n",
        "    \"MNIST\", \"FashionMNIST\", \"KMNIST\", \"EMNIST\", \"QMNIST\", \"USPS\",\n",
        "    \"CIFAR10\", \"CIFAR100\", \"SVHN\", \"STL10\"\n",
        "]\n",
        "\n",
        "optimizers = [\n",
        "    \"Adam\", \"AdamW\", \"RMSprop\", \"Lion\",\n",
        "    \"MuAdam\", \"ThermoLion\", # Renamed here\n",
        "    \"Lookahead\", \"SWATS\", \"SWA\"\n",
        "]\n",
        "\n",
        "results = {ds: {opt: {'loss': [], 'metric': 0.0, 'final_loss': 0.0} for opt in optimizers} for ds in datasets}\n",
        "\n",
        "print(f\"Starting Ultimate Image Benchmark (12 Datasets)...\")\n",
        "print(f\"Optimizers: {', '.join(optimizers)}\")\n",
        "print(\"-\" * 110)\n",
        "\n",
        "for ds in datasets:\n",
        "    print(f\"Processing {ds}...\")\n",
        "    for opt in optimizers:\n",
        "        try:\n",
        "            loss_curve, duration, metric, final_loss = train_and_evaluate(ds, opt)\n",
        "            results[ds][opt]['loss'] = loss_curve\n",
        "            results[ds][opt]['metric'] = metric\n",
        "            results[ds][opt]['final_loss'] = final_loss\n",
        "            print(f\"  > {opt:<12} | Loss: {final_loss:.4f} | Acc: {metric:.2f}% | Time: {duration:.2f}s\")\n",
        "        except Exception as e:\n",
        "            print(f\"  > {opt} Failed on {ds}: {e}\")\n",
        "\n",
        "# ==============================================\n",
        "# ====== PLOTTING & REPORTING ==================\n",
        "# ==============================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*140)\n",
        "print(f\"{'FINAL ACCURACY SUMMARY':^140}\")\n",
        "print(\"=\"*140)\n",
        "header = f\"{'Dataset':<15} | \"\n",
        "for opt in optimizers:\n",
        "    header += f\"{opt[:8]:<10} | \"\n",
        "print(header)\n",
        "print(\"-\" * len(header))\n",
        "\n",
        "for ds in datasets:\n",
        "    row = f\"{ds:<15} | \"\n",
        "    for opt in optimizers:\n",
        "        val = results[ds][opt]['metric']\n",
        "        row += f\"{val:5.1f}      | \"\n",
        "    print(row)\n",
        "print(\"=\"*140 + \"\\n\")\n",
        "\n",
        "# Plotting with Increased Font Sizes\n",
        "plt.rcParams.update({'font.size': 14}) # Global increase\n",
        "\n",
        "cols = 3\n",
        "rows = (len(datasets) + cols - 1) // cols\n",
        "fig, axes = plt.subplots(rows, cols, figsize=(24, 6.0 * rows))\n",
        "axes = axes.flatten()\n",
        "colors = {opt: plt.get_cmap('tab10')(i) for i, opt in enumerate(optimizers)}\n",
        "\n",
        "for i, ds in enumerate(datasets):\n",
        "    ax = axes[i]\n",
        "    ax.set_title(f\"{ds}\", fontsize=16, fontweight='bold')\n",
        "    for opt in optimizers:\n",
        "        curve = results[ds][opt]['loss']\n",
        "        if len(curve) > 1:\n",
        "            ax.plot(curve, label=opt, color=colors.get(opt, 'black'), linewidth=2.0)\n",
        "\n",
        "    # Larger Tick Labels\n",
        "    ax.tick_params(axis='both', which='major', labelsize=14)\n",
        "    ax.grid(True, alpha=0.3)\n",
        "\n",
        "    # Legend only on first plot to avoid clutter\n",
        "    if i == 0:\n",
        "        ax.legend(fontsize=14, ncol=2)\n",
        "\n",
        "    try: ax.set_yscale('log')\n",
        "    except: pass\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    }
  ]
}